\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\caption{Operator-wise latency breakdown during prefill phase (device-side CUDA time). Results report the total elapsed time for each operator category. }
\begin{tabular}{l c c
                r r
                r r
                r r
                r r}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{BS}} & \multirow{2}{*}{\textbf{Seqlen}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*mm}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::softmax}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*norm}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*lu}}} \\
\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
 &  &  & \multicolumn{1}{c}{\textbf{Time (ms)}} &  \multicolumn{1}{c}{\textbf{Time (\%)}}  & \multicolumn{1}{c}{\textbf{Time (ms)}} &  \multicolumn{1}{c}{\textbf{Time (\%)}}  & \multicolumn{1}{c}{\textbf{Time (ms)}} &  \multicolumn{1}{c}{\textbf{Time (\%)}}  & \multicolumn{1}{c}{\textbf{Time (ms)}} &  \multicolumn{1}{c}{\textbf{Time (\%)}}  \\
\midrule
\multirow{9}{*}{Llama-3.2-3B} & \multirow{3}{*}{1} & 512  & 11.095  & 5.20\%  & 1.537   & 0.72\% & 6.393  & 3.00\% & 0.346 & 0.16\% \\
&  & 1024 & 21.057  & 11.42\% & 6.308   & 3.42\% & 5.961  & 3.23\% & 0.568 & 0.31\% \\
&  & 2048 & 44.245  & 9.55\%  & 25.092  & 5.42\% & 7.331  & 1.58\% & 1.178 & 0.25\% \\ \cmidrule(lr){3-11}
&  \multirow{3}{*}{4} & 512  & 38.365  & 14.21\% & 6.141   & 2.27\% & 7.387  & 2.74\% & 1.181 & 0.44\% \\
&  & 1024 & 78.264  & 12.43\% & 23.104  & 3.67\% & 13.196 & 2.10\% & 2.405 & 0.38\% \\
&  & 2048 & 173.133 & 9.76\%  & 97.943  & 5.52\% & 23.553 & 1.33\% & 4.839 & 0.27\% \\ \cmidrule(lr){3-11}
&  \multirow{3}{*}{8} & 512  & 74.146  & 14.51\% & 11.730  & 2.30\% & 13.072 & 2.56\% & 2.405 & 0.47\% \\
&  & 1024 & 155.640 & 12.70\% & 45.512  & 3.71\% & 23.509 & 1.92\% & 4.817 & 0.39\% \\
&  & 2048 & 347.385 & 9.85\%  & 194.505 & 5.51\% & 45.172 & 1.28\% & 9.737 & 0.28\% \\
\midrule
\multirow{9}{*}{bert-base}  & \multirow{3}{*}{1} & 128 & 5.687  & 15.83\% & 0.178 & 0.50\% & 0.317 & 0.88\% & 0.051 & 0.14\% \\
& & 256 & 6.866  & 20.32\% & 0.217 & 0.64\% & 0.335 & 0.99\% & 0.071 & 0.21\% \\
& & 512 & 10.520 & 24.93\% & 0.557 & 1.32\% & 0.359 & 0.85\% & 0.096 & 0.23\% \\ \cmidrule(lr){3-11}
&  \multirow{3}{*}{4}  & 128 & 9.548  & 24.42\% & 0.222 & 0.57\% & 0.356 & 0.91\% & 0.093 & 0.24\% \\
&  & 256 & 15.387 & 26.37\% & 0.487 & 0.83\% & 0.426 & 0.73\% & 0.165 & 0.28\% \\
&  & 512 & 26.139 & 25.08\% & 1.705 & 1.64\% & 0.693 & 0.67\% & 0.379 & 0.36\% \\ \cmidrule(lr){3-11}
&  \multirow{3}{*}{8}  & 128 & 14.798 & 27.59\% & 0.320 & 0.60\% & 0.426 & 0.79\% & 0.162 & 0.30\% \\
&  & 256 & 24.311 & 29.48\% & 0.864 & 1.05\% & 0.691 & 0.84\% & 0.387 & 0.47\% \\
&  & 512 & 49.772 & 29.65\% & 3.051 & 1.82\% & 1.065 & 0.63\% & 0.792 & 0.47\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
% \footnotesize
\item \textit{Note:} \texttt{aten::mm*} aggregates all matrix-multiplication-like operators, such as \texttt{mm}, \texttt{addmm}, \texttt{bmm}, \texttt{matmul}, which constitute the underlying primitives used by linear layers. Similarly, \texttt{aten::norm*} includes normalization operators such as \texttt{layer\_norm}, \texttt{rms\_norm}, etc. \texttt{aten::*lu} includes activation functions such as \texttt{aten::silu} and \texttt{aten::gelu}. 
\end{tablenotes}
\label{tab:profile-prefill}
\end{threeparttable}}
\end{table}

\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\caption{Operator-wise latency breakdown during decode phase (device-side CUDA time). Results report the total elapsed time for each operator category.}
\begin{tabular}{l c 
                r r
                r r
                r r
                r r}
\toprule
\multirow{2}{*}{\textbf{Model}}  & \multirow{2}{*}{\textbf{Seqlen}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*mm}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::softmax}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*norm}}}
& \multicolumn{2}{c}{\textbf{\texttt{aten::*lu}}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}
 &  & \textbf{Time (ms)} & \textbf{Time (\%)} & \textbf{Time (ms)} & \textbf{Time (\%)} & \textbf{Time (ms)} & \textbf{Time (\%)} & \textbf{Time (ms)} & \textbf{Time (\%)} \\
\midrule
\multirow{4}{*}{Llama-3.2-3B}  & 128  & 6.336 & 9.40\% & 0.129 & 0.19\% & 4.154 & 6.16\% & 0.120 & 0.18\% \\
&  256  & 6.333 & 9.68\% & 0.129 & 0.20\% & 4.071 & 6.22\% & 0.119 & 0.18\% \\
&  512  & 6.358 & 9.69\% & 0.130 & 0.20\% & 3.959 & 6.03\% & 0.121 & 0.18\% \\
&  1024 & 6.356 & 9.47\% & 0.130 & 0.19\% & 4.072 & 6.07\% & 0.121 & 0.18\% \\
\midrule
\multirow{4}{*}{bert-base}  &  128  & 1.439 & 8.64\% & 0.079 & 0.48\% & 0.304 & 1.83\% & 0.047 & 0.28\% \\
&  256  & 1.438 & 8.60\% & 0.079 & 0.48\% & 0.304 & 1.82\% & 0.047 & 0.28\% \\
&  512  & 1.445 & 7.07\% & 0.079 & 0.39\% & 0.304 & 1.49\% & 0.047 & 0.23\% \\
&  1024 & 1.441 & 8.20\% & 0.079 & 0.45\% & 0.304 & 1.73\% & 0.047 & 0.27\% \\
\bottomrule
\end{tabular}
\begin{tablenotes}
% \footnotesize
\item \textit{Note:} \texttt{aten::mm*} aggregates all matrix-multiplication-like operators, such as \texttt{mm}, \texttt{addmm}, \texttt{bmm}, \texttt{matmul}, which constitute the underlying primitives used by linear layers. Similarly, \texttt{aten::norm*} includes normalization operators such as \texttt{layer\_norm}, \texttt{rms\_norm}, etc. \texttt{aten::*lu} includes activation functions such as \texttt{aten::silu} and \texttt{aten::gelu}. 
\end{tablenotes}
\label{tab:profile-decode}
\end{threeparttable}
}
\end{table}

