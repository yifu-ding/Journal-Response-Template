%====================== Prefill Benchmark ======================%
\begin{table*}[ht]
\centering
\caption{Prefilling benchmark on various models with $<$3B parameters. All experiments are tested on H800 with 96G VRAM. And the input token length are uniformly set to $2k$. }
\label{tab:prefill-bench}
\small
\begin{tabular}{lcccrrr}
\toprule
\textbf{Models}  &  \textbf{W$_{bit}$/A$_{bit}$} & \textbf{Storage (GB)} & \textbf{BS} & \textbf{Time (s)} & \textbf{Tokens/s} & \textbf{Mem. (GB)} \\
\midrule
\multirow{3}{*}{Llama-3.2-3B}   & \multirow{3}{*}{4/16 (bnb)} &  \multirow{3}{*}{2.240}  & 1 & 0.072 & 28267 & 15.59  \\
&  & & 4 & 0.172 & 11924 & 54.94 \\
&  & & 8 & --      & --          & OOM      \\  \midrule
\multirow{6}{*}{Gemma-2B}     & \multirow{3}{*}{16/16}  &  \multirow{3}{*}{5.012} & 1 & 0.063 & 32492 & 20.75    \\
&    & & 4 & 0.116 & 17694 & 64.51 \\
&    & & 8 & --      & --          & OOM      \\  \cmidrule{2-7}
& \multirow{3}{*}{4/16 \makecell[c]{(auto \\ gptq)}}  & \multirow{3}{*}{3.130}  & 1  & 0.107 & 19133 & 22.53 \\
& & & 4  & 0.173 & 11858 & 57.71 \\
& & & 8  & --      & --          & OOM      \\ \midrule
\multirow{4}{*}{Bitnet-b1.58-2B} & \multirow{4}{*}{1.5/8}  &  \multirow{4}{*}{1.835} & 1  & 0.026 & 77253  & 10.56 \\
&   & & 4  & 0.095 & 21508  & 19.60 \\
&  & & 8  & 0.189 & 10814  & 31.67 \\
&  & & 16 & 0.378 & 5415  & 55.60 \\  \midrule
% \multirow{4}{*}{\makecell[l]{\textbf{BWTA-2B (Ours)} \\ \textit{(replace 6\% param.)}} } & \multirow{4}{*}{1.47/7.61}  & \multirow{4}{*}{\textbf{1.819}} &  1  & \textbf{0.027} & \textbf{77225} & \textbf{10.28} \\
% &  & & 4  & \textbf{0.093} & \textbf{21995} & \textbf{19.44} \\
% &  & & 8  & \textbf{0.185} & \textbf{11096} & \textbf{31.48} \\
% &  & & 16 & \textbf{0.369} &  \textbf{5544} & \textbf{55.41} \\   \midrule
\multirow{4}{*}{\makecell[l]{\textbf{BWTA-2B (Ours)}\\ \textit{(replace 10\% param.)}} }  & \multirow{4}{*}{1.45/7.35}  & \multirow{4}{*}{\textbf{1.809}} & 1  & \textbf{0.026} & \textbf{80282} & \textbf{10.18} \\
&  & & 4  & \textbf{0.091} & \textbf{22451} & \textbf{19.30} \\
&  & & 8  & \textbf{0.182} & \textbf{11271} & \textbf{31.31} \\
&  & & 16 & \textbf{0.364} &  \textbf{5631} & \textbf{55.25} \\ \midrule
\multirow{4}{*}{\makecell[l]{\textbf{BWTA-2B (Ours)}\\ \textit{(replace 15\% param.)}} }  & \multirow{4}{*}{1.42/7.03} & \multirow{4}{*}{\textbf{1.796}}  & 1  & \textbf{0.025} & \textbf{81529} & \textbf{10.10} \\
& & & 4  & \textbf{0.090} & \textbf{22723} & \textbf{19.55} \\
& & & 8  & \textbf{0.179} & \textbf{11466} & \textbf{32.02} \\
& & & 16 & \textbf{0.356} &  \textbf{5745} & \textbf{56.94} \\
\bottomrule
\end{tabular}
\end{table*}

% We use the following LLM pretrained models: 
% \begin{itemize}
%     \item Llama-3.2-3B with INT4 quantization\footnote{https://huggingface.co/unsloth/Llama-3.2-3B-bnb-4bit} provided by \textit{unsloth}.
%     \item Gemma-2B with BF16\footnote{https://huggingface.co/google/gemma-2b} provided by \textit{Google}. 
%     \item  Gemma-2B with INT4 quantization using AutoGPTQ\footnote{https://huggingface.co/Intel/gemma-2b-int4-inc} provided by \textit{Intel}. 
%     \item Bitnet-b1.58-2B with ternary weight and INT8 quantization\footnote{https://huggingface.co/microsoft/bitnet-b1.58-2B-4T} provided by \textit{Microsoft}. 
% \end{itemize}
